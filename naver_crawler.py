# ===== ìµœìš°ì„  ë¡œê¹… ì„¤ì • (import ì˜¤ë¥˜ë„ ìºì¹˜) =====
import os
import sys
import logging

# ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì •
_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
_LOG_FILE = os.path.join(_SCRIPT_DIR, "crawler_log.txt")

# ê¸°ë³¸ ë¡œê¹… ì„¤ì • (import ì „ì— ì„¤ì •)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(_LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
_logger = logging.getLogger("startup")
_logger.info("=" * 60)
_logger.info("ìŠ¤í¬ë¦½íŠ¸ ë¡œë“œ ì‹œì‘")
_logger.info(f"ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
_logger.info(f"ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ: {_SCRIPT_DIR}")

# ===== ëª¨ë“ˆ Import (ì˜¤ë¥˜ ì‹œ ë¡œê·¸ì— ê¸°ë¡) =====
try:
    import requests
    from bs4 import BeautifulSoup
    import urllib.parse
    import time
    import random
    import re
    import signal
    from datetime import datetime, timedelta
    
    from podcast_generator import generate_podcast_script
    from podcast_audio import run_audio_generation
    from sftp_uploader import upload_file
    import db_manager
    _logger.info("ëª¨ë“  ëª¨ë“ˆ import ì„±ê³µ")
except Exception as e:
    _logger.error(f"ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    import traceback
    _logger.error(traceback.format_exc())
    sys.exit(1)

logger = _logger  # ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±

# User-Agent ëª©ë¡ (ëœë¤ ì„ íƒìœ¼ë¡œ ì°¨ë‹¨ ë°©ì§€)
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
]

def get_random_headers():
    """ëœë¤ User-Agentë¥¼ í¬í•¨í•œ í—¤ë” ë°˜í™˜"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
    }

# Graceful Shutdown í”Œë˜ê·¸
_shutdown_requested = False

def signal_handler(signum, frame):
    """Ctrl+C ë“± ì‹œê·¸ë„ ì²˜ë¦¬"""
    global _shutdown_requested
    print("\nâš ï¸ ì¢…ë£Œ ìš”ì²­ ê°ì§€. í˜„ì¬ ì‘ì—… ì™„ë£Œ í›„ ì¢…ë£Œí•©ë‹ˆë‹¤...")
    _shutdown_requested = True

def crawl_naver_news(query, keyword_id=None, requirements=None, use_ai=True, make_audio=True, max_articles=3):
    # Encode the query for the URL
    encoded_query = urllib.parse.quote(query)
    
    # Base URL provided by the user
    # Note: query parameter is replaced with the user input
    url = f"https://search.naver.com/search.naver?ssc=tab.news.all&query={encoded_query}&sm=tab_opt&sort=1&nso=so%3Add"
    
    headers = get_random_headers()
    
    # Statistics tracking
    stats = {
        'total': 0,
        'success': 0,
        'duplicate': 0,
        'failed': 0
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Naver News search results list
        news_list_ul = soup.select_one("ul.list_news")
        
        print(f"ê²€ìƒ‰ì–´ '{query}'ì— ëŒ€í•œ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.\n")
        
        if not news_list_ul:                        
            print("ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return stats


        # Find headlines with fallback strategies (ë‚´êµ¬ì„± í–¥ìƒ)
        headline_selectors = [
            lambda c: c and 'sds-comps-text-type-headline1' in c,  # Current selector
            lambda c: c and 'news_tit' in c,  # Legacy selector
            lambda c: c and 'title' in c.lower() and 'news' in c.lower(),  # Generic
        ]
        
        headlines = []
        for selector in headline_selectors:
            headlines = news_list_ul.find_all(class_=selector)
            if headlines:
                print(f"âœ“ í—¤ë“œë¼ì¸ ë°œê²¬ (ì…€ë ‰í„° ì „ëµ ì‚¬ìš©)")
                break

        if not headlines:
             print("ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
             return stats

        # Limit to top N articles per keyword to avoid spamming
        for i, headline in enumerate(headlines[:max_articles]):
            stats['total'] += 1
            
            try:
                # Title
                title = headline.get_text(strip=True)
                
                # Link (parent a tag)
                link_el = headline.find_parent("a")
                link = link_el['href'] if link_el else ""
                
                # Check for duplicates
                if link and db_manager.is_duplicate_news(link):
                    print(f"[ì¤‘ë³µ ê±´ë„ˆë›°ê¸°] {title}")
                    stats['duplicate'] += 1
                    continue
                
                
                # Press (closest preceding press element) with fallback
                press_selectors = [
                    lambda c: c and 'sds-comps-profile-info-title-text' in c,
                    lambda c: c and 'press' in c.lower(),
                    lambda c: c and 'info_group' in c,
                ]
                
                press = "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"
                for press_selector in press_selectors:
                    press_el = headline.find_previous(class_=press_selector)
                    if press_el:
                        press = press_el.get_text(strip=True)
                        break

                print(f"ì–¸ë¡ ì‚¬: {press}")
                print(f"ì œëª©: {title}")
                print(f"ë§í¬: {link}")
                
                
                content = ""
                if link:
                    # Rate limiting: Random delay to avoid IP blocking
                    delay = random.uniform(1.0, 3.0)
                    print(f"ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì¤‘... (ëŒ€ê¸°: {delay:.1f}ì´ˆ)")
                    time.sleep(delay)
                    content = get_news_content(link)
                    # print(f"ë³¸ë¬¸:\n{content}") # Too verbose
                
                if use_ai and content and "ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" not in content:
                    print("\n[AI íŒŸìºìŠ¤íŠ¸ ëŒ€ë³¸ ìƒì„± ì¤‘...]")
                    script = generate_podcast_script(title, content, requirements=requirements)
                    print(f"--- íŒŸìºìŠ¤íŠ¸ ëŒ€ë³¸ ---\n{script[:200]}...\n---------------------")
                    
                    if make_audio:
                        print("[ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ì¤‘...]")
                        
                        # Ensure MP3 directory exists
                        if not os.path.exists("MP3"):
                            os.makedirs("MP3")
                            
                        # Create a safe filename
                        safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '-', '_')]).strip()[:30]
                        filename = os.path.join("MP3", f"podcast_{safe_title}_{i}.mp3")
                        
                        # Pass title to audio generator for announcement
                        audio_result = run_audio_generation(script, filename, title=title)
                        
                        # Check if audio was successfully generated
                        if not audio_result:
                            print("âŒ ì˜¤ë””ì˜¤ ìƒì„± ì‹¤íŒ¨ - ìœ íš¨í•œ ëŒ€ë³¸ì´ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œ ë° DB ì €ì¥ ê±´ë„ˆëœ€.")
                            stats['failed'] += 1
                        else:
                            # âœ… íŒŒì¼ í¬ê¸° ì´ì¤‘ ê²€ì¦ (ì•ˆì „ì¥ì¹˜)
                            try:
                                file_size = os.path.getsize(filename)
                                file_size_mb = file_size / (1024 * 1024)
                                
                                if file_size < 1048576:  # 1MB = 1048576 bytes
                                    print(f"âŒ íŒŒì¼ í¬ê¸° ë¶€ì¡±: {file_size_mb:.2f}MB (ìµœì†Œ 1MB í•„ìš”)")
                                    print(f"   ì—…ë¡œë“œ ë° DB ë“±ë¡ ê±´ë„ˆëœ€")
                                    if os.path.exists(filename):
                                        os.remove(filename)
                                        print(f"   ë¡œì»¬ íŒŒì¼ ì‚­ì œ: {filename}")
                                    stats['failed'] += 1
                                    continue
                                
                                print(f"âœ… íŒŒì¼ í¬ê¸° ê²€ì¦ í†µê³¼: {file_size_mb:.2f}MB")
                            except Exception as e:
                                print(f"âŒ íŒŒì¼ í¬ê¸° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                                stats['failed'] += 1
                                continue
                            
                            print("[ì„œë²„ë¡œ ì—…ë¡œë“œ ì¤‘...]")
                            remote_path = upload_file(filename)
                            
                            if remote_path:
                                print(f"[DB ì €ì¥ ì¤‘...] {title}")
                                db_manager.insert_episode(press, title, link, remote_path, keyword_id=keyword_id)
                                
                                # Clean up local file after successful upload
                                try:
                                    os.remove(filename)
                                    print(f"[ë¡œì»¬ íŒŒì¼ ì‚­ì œ] {filename}")
                                except Exception as e:
                                    print(f"[ë¡œì»¬ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨] {e}")
                                
                                stats['success'] += 1
                            else:
                                print("[ì—…ë¡œë“œ ì‹¤íŒ¨] ë¡œì»¬ íŒŒì¼ ìœ ì§€")
                                stats['failed'] += 1
                else:
                    print("[ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” AI ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°]")
                    stats['failed'] += 1

                print("-" * 50)
                
            except Exception as e:
                print(f"[ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜] {e}")
                stats['failed'] += 1
                print("-" * 50)
                continue  # Move to next article
            
    except requests.exceptions.RequestException as e:
        print(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    # Print statistics
    print(f"\nğŸ“Š í¬ë¡¤ë§ í†µê³„ - ì´: {stats['total']}, ì„±ê³µ: {stats['success']}, ì¤‘ë³µ: {stats['duplicate']}, ì‹¤íŒ¨: {stats['failed']}\n")
    return stats

def validate_content(content):
    """Validate article content quality."""
    if not content or len(content) < 200:
        return False
    
    # Check Korean character ratio
    korean_chars = len([c for c in content if 'ê°€' <= c <= 'í£'])
    total_chars = len(content.replace('\n', '').replace(' ', ''))
    
    if total_chars == 0:
        return False
    
    if korean_chars / total_chars < 0.3:  # At least 30% Korean
        return False
    
    return True


def clean_article_text(text):
    """Remove reporter info, copyright, and other unwanted patterns from article text."""
    if not text:
        return text
    
    # ê¸°ì ì´ë©”ì¼ ì œê±°
    text = re.sub(r'\w+@\w+\.\w+', '', text)
    
    # ê¸°ì ì„œëª… íŒ¨í„´ ì œê±°
    reporter_patterns = [
        r'.*?ê¸°ì\s*=\s*',
        r'.*?íŠ¹íŒŒì›\s*=\s*', 
        r'\[.*?ê¸°ì\]',
        r'ê¸°ì\s+\w+',
    ]
    for pattern in reporter_patterns:
        text = re.sub(pattern, '', text)
    
    # ì €ì‘ê¶Œ/ì¶œì²˜ ê´€ë ¨ ì œê±°
    copyright_patterns = [
        r'Copyright\s*Â©.*',
        r'ì €ì‘ê¶Œì.*',
        r'ë¬´ë‹¨\s*ì „ì¬.*',
        r'ë°°í¬\s*ê¸ˆì§€.*',
        r'â“’.*',
    ]
    for pattern in copyright_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # SNS ê³µìœ  ê´€ë ¨ ì œê±°
    text = re.sub(r'(ì¹´ì¹´ì˜¤í†¡|í˜ì´ìŠ¤ë¶|íŠ¸ìœ„í„°|ê³µìœ í•˜ê¸°).*', '', text)
    
    # ===== ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¹ìœ  ë¶ˆí•„ìš” í…ìŠ¤íŠ¸ ì œê±° =====
    naver_noise_patterns = [
        r'ê¸°ì‚¬ ì„¹ì…˜ ë¶„ë¥˜ ì•ˆë‚´.*?ìˆìŠµë‹ˆë‹¤\.',  # ì„¹ì…˜ ë¶„ë¥˜ ì•ˆë‚´
        r'ì´ ê¸°ì‚¬ëŠ” ì–¸ë¡ ì‚¬ì—ì„œ.*?ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤\.',  # ì–¸ë¡ ì‚¬ ë¶„ë¥˜ ì•ˆë‚´
        r'ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜í–ˆìŠµë‹ˆë‹¤',
        r'.*?ë°”ë¡œê°€ê¸°\n?',  # ë°”ë¡œê°€ê¸° ë§í¬
        r'ê¸°ì‚¬ì˜ ì„¹ì…˜ ì •ë³´ëŠ”.*',  # ì„¹ì…˜ ì •ë³´ ì•ˆë‚´
        r'ì–¸ë¡ ì‚¬ëŠ” ê°œë³„ ê¸°ì‚¬ë¥¼.*',  # ì–¸ë¡ ì‚¬ ì•ˆë‚´
        r'\[.*?ë‰´ìŠ¤\]',  # [OOë‰´ìŠ¤] íŒ¨í„´
        r'ã€.*?ã€‘',  # ã€ê¸°ì‚¬ã€‘ íŒ¨í„´
        r'â–¶.*?\n?',  # â–¶ ê´€ë ¨ê¸°ì‚¬ ë“±
        r'â—†.*?\n?',  # â—† ê´€ë ¨ê¸°ì‚¬ ë“±
        r'â– .*?\n?',  # â–  ê´€ë ¨ê¸°ì‚¬ ë“±
        r'â˜.*?\n?',  # â˜ ê´€ë ¨ê¸°ì‚¬ ë“±
        r'â–·.*?\n?',  # â–· ê´€ë ¨ê¸°ì‚¬ ë“±
        r'ì‚¬ì§„=.*?\n?',  # ì‚¬ì§„ ì¶œì²˜
        r'\(ì‚¬ì§„.*?\)',  # (ì‚¬ì§„ ì„¤ëª…)
        r'ì˜ìƒ=.*?\n?',  # ì˜ìƒ ì¶œì²˜
    ]
    for pattern in naver_noise_patterns:
        text = re.sub(pattern, '', text, flags=re.DOTALL)
    
    # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def get_news_content(url):
    """
    Extract news article content with multi-layer fallback strategies.
    
    ì „ëµ ìˆœì„œ:
    1. ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© ì…€ë ‰í„° (ìµœì‹  êµ¬ì¡° ë°˜ì˜)
    2. ì£¼ìš” ì–¸ë¡ ì‚¬ë³„ ë§ì¶¤ ì…€ë ‰í„°
    3. ê³µí†µ ë‰´ìŠ¤ ì…€ë ‰í„°
    4. <article> íƒœê·¸
    5. <p> íƒœê·¸ íœ´ë¦¬ìŠ¤í‹±
    6. meta description í´ë°±
    """
    extraction_log = []  # ë””ë²„ê¹…ìš© ë¡œê·¸
    
    try:
        headers = get_random_headers()
        response = requests.get(url, headers=headers, timeout=15, allow_redirects=True)
        response.raise_for_status()
        
        # Detect encoding
        response.encoding = response.apparent_encoding
        final_url = response.url  # ë¦¬ë‹¤ì´ë ‰íŠ¸ í›„ ìµœì¢… URL
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ===== ë¶ˆí•„ìš” ìš”ì†Œ ì œê±° (ê°•í™”) =====
        for unwanted in soup([
            "script", "style", "header", "footer", "nav", 
            "iframe", "noscript", "aside", "figure", "figcaption"
        ]):
            unwanted.decompose()
        
        # ê´‘ê³ , ë°°ë„ˆ, ê´€ë ¨ê¸°ì‚¬ ì œê±°
        unwanted_patterns = [
            'ad', 'banner', 'sidebar', 'related', 'recommend',
            'comment', 'sns', 'share', 'copyright', 'journalist',
            'byline', 'photo_org', 'sponsor', 'modal', 'popup'
        ]
        for pattern in unwanted_patterns:
            for element in soup.find_all(class_=lambda c: c and pattern in c.lower()):
                element.decompose()
            for element in soup.find_all(id=lambda i: i and pattern in i.lower()):
                element.decompose()
        
        # ê´€ë ¨ê¸°ì‚¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ ì œê±°
        related_keywords = ['ê´€ë ¨ê¸°ì‚¬', 'í•¨ê»˜ ë³´ë©´', 'ì´ì „ ê¸°ì‚¬', 'ë‹¤ìŒ ê¸°ì‚¬', 'ì¶”ì²œ ê¸°ì‚¬', 'ì¸ê¸°ê¸°ì‚¬', 'ë§ì´ ë³¸']
        for element in soup.find_all(['div', 'section', 'aside', 'ul']):
            if element.get_text():
                text_sample = element.get_text()[:100]
                if any(keyword in text_sample for keyword in related_keywords):
                    element.decompose()
        
        # ===== Strategy 1: ë„¤ì´ë²„ ë‰´ìŠ¤ ì „ìš© (ìµœì‹  êµ¬ì¡°) =====
        if 'naver.com' in final_url:
            naver_selectors = [
                '#dic_area',              # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ (ìµœì‹ )
                '#newsct_article',        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ (êµ¬ë²„ì „)
                '#articeBody',            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ (êµ¬ë²„ì „2)
                '#articleBodyContents',   # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ (êµ¬ë²„ì „3)
                '.newsct_article',        # í´ë˜ìŠ¤ ë²„ì „
                '._article_body',         # ëª¨ë°”ì¼ ë²„ì „
                '#content',               # ì¼ë°˜ì  ì»¨í…ì¸  ì˜ì—­
            ]
            for selector in naver_selectors:
                element = soup.select_one(selector)
                if element:
                    text = element.get_text(separator='\n', strip=True)
                    text = clean_article_text(text)
                    if validate_content(text):
                        extraction_log.append(f"âœ“ ë„¤ì´ë²„ ì…€ë ‰í„° ì„±ê³µ: {selector}")
                        print(f"  [ì¶”ì¶œ] ë„¤ì´ë²„ ë‰´ìŠ¤ ({selector})")
                        return text
                    extraction_log.append(f"âœ— ë„¤ì´ë²„ ì…€ë ‰í„° ê²€ì¦ ì‹¤íŒ¨: {selector}")
        
        # ===== Strategy 2: ì£¼ìš” ì–¸ë¡ ì‚¬ë³„ ë§ì¶¤ ì…€ë ‰í„° =====
        press_selectors = {
            # ì¡°ì„ ì¼ë³´
            'chosun.com': ['#article-view-content-div', '.article-body', '#articleBody'],
            # ì¤‘ì•™ì¼ë³´
            'joongang.co.kr': ['#article_body', '.article_body', '#article-content'],
            # ë™ì•„ì¼ë³´
            'donga.com': ['#article_body', '.article_body', '.article_txt'],
            # í•œê²¨ë ˆ
            'hani.co.kr': ['.article-body', '.article-text', '#article-body'],
            # ê²½í–¥ì‹ ë¬¸
            'khan.co.kr': ['.art_body', '.article_body', '#articleBody'],
            # í•œêµ­ê²½ì œ
            'hankyung.com': ['#articletxt', '.article-body', '#article-body-view'],
            # ë§¤ì¼ê²½ì œ
            'mk.co.kr': ['#article_body', '.art_txt', '.article_body'],
            # ì—°í•©ë‰´ìŠ¤
            'yna.co.kr': ['.article-txt', '.story-news', '#articleWrap'],
            # SBS
            'sbs.co.kr': ['.article_cont_area', '.text_area', '#news_content'],
            # KBS
            'kbs.co.kr': ['.detail-body', '.article-body', '#cont_newstext'],
            # MBC
            'mbc.co.kr': ['.news_txt', '.article_body', '.article-body'],
            # JTBC
            'jtbc.co.kr': ['.article_content', '.article-content', '#article-body'],
            # YTN
            'ytn.co.kr': ['.paragraph', '.article-body', '#CmAdContent'],
            # ë‰´ì‹œìŠ¤
            'newsis.com': ['.article_body', '.viewer', '#articleBody'],
            # ë‰´ìŠ¤1
            'news1.kr': ['.article', '.article_body', '#news-body'],
            # ì´ë°ì¼ë¦¬
            'edaily.co.kr': ['.news_body', '.article_body', '#articleBody'],
            # ë¨¸ë‹ˆíˆ¬ë°ì´
            'mt.co.kr': ['#textBody', '.article-body', '.textBody'],
            # ì„œìš¸ê²½ì œ
            'sedaily.com': ['#v-article-content', '.article_view', '.article_body'],
            # ì˜¤ë§ˆì´ë‰´ìŠ¤
            'ohmynews.com': ['.article_body', '.at_contents', '#articleBody'],
            # í”„ë ˆì‹œì•ˆ
            'pressian.com': ['.article_body', '.article-body', '#news_body'],
        }
        
        for domain, selectors in press_selectors.items():
            if domain in final_url:
                for selector in selectors:
                    element = soup.select_one(selector)
                    if element:
                        text = element.get_text(separator='\n', strip=True)
                        text = clean_article_text(text)
                        if validate_content(text):
                            extraction_log.append(f"âœ“ ì–¸ë¡ ì‚¬ ì…€ë ‰í„° ì„±ê³µ: {domain} - {selector}")
                            print(f"  [ì¶”ì¶œ] {domain} ({selector})")
                            return text
                        extraction_log.append(f"âœ— ì–¸ë¡ ì‚¬ ì…€ë ‰í„° ê²€ì¦ ì‹¤íŒ¨: {domain} - {selector}")
                break  # í•´ë‹¹ ì–¸ë¡ ì‚¬ ì…€ë ‰í„° ì‹œë„ í›„ ë‹¤ìŒ ì „ëµìœ¼ë¡œ
        
        # ===== Strategy 3: ê³µí†µ ë‰´ìŠ¤ ì…€ë ‰í„° =====
        common_selectors = [
            'article',
            '[itemprop="articleBody"]',
            '.article-body',
            '.article_body', 
            '.article-content',
            '.article_content',
            '.news-body',
            '.news_body',
            '.post-content',
            '.entry-content',
            '#article-body',
            '#article_body',
            '#articleBody',
            '#content-body',
            '.story-body',
            '.text-body',
        ]
        
        for selector in common_selectors:
            element = soup.select_one(selector)
            if element:
                text = element.get_text(separator='\n', strip=True)
                text = clean_article_text(text)
                if validate_content(text):
                    extraction_log.append(f"âœ“ ê³µí†µ ì…€ë ‰í„° ì„±ê³µ: {selector}")
                    print(f"  [ì¶”ì¶œ] ê³µí†µ ì…€ë ‰í„° ({selector})")
                    return text
                extraction_log.append(f"âœ— ê³µí†µ ì…€ë ‰í„° ê²€ì¦ ì‹¤íŒ¨: {selector}")
        
        # ===== Strategy 4: <p> íƒœê·¸ íœ´ë¦¬ìŠ¤í‹± (ê°•í™”) =====
        paragraphs = soup.find_all('p')
        content = []
        for p in paragraphs:
            text = p.get_text(strip=True)
            # ë³¸ë¬¸ ê°€ëŠ¥ì„± ë†’ì€ ë¬¸ë‹¨ë§Œ ìˆ˜ì§‘ (ìµœì†Œ 50ì, ë¬¸ì¥ í˜•íƒœ)
            if len(text) > 50 and ('ë‹¤.' in text or 'ìš”.' in text or 'ì£ .' in text):
                content.append(text)
        
        if content:
            combined = "\n".join(content)
            combined = clean_article_text(combined)
            if validate_content(combined):
                extraction_log.append(f"âœ“ <p> íƒœê·¸ íœ´ë¦¬ìŠ¤í‹± ì„±ê³µ ({len(content)}ê°œ ë¬¸ë‹¨)")
                print(f"  [ì¶”ì¶œ] <p> íƒœê·¸ íœ´ë¦¬ìŠ¤í‹± ({len(content)}ê°œ ë¬¸ë‹¨)")
                return combined
            extraction_log.append(f"âœ— <p> íƒœê·¸ íœ´ë¦¬ìŠ¤í‹± ê²€ì¦ ì‹¤íŒ¨")
        
        # ===== Strategy 5: ê°€ì¥ ê¸´ í…ìŠ¤íŠ¸ ë¸”ë¡ ì°¾ê¸° (ìµœí›„ íœ´ë¦¬ìŠ¤í‹±) =====
        all_divs = soup.find_all('div')
        best_text = ""
        best_score = 0
        
        for div in all_divs:
            text = div.get_text(separator='\n', strip=True)
            # ì ìˆ˜ ê³„ì‚°: ê¸¸ì´ + í•œê¸€ ë¹„ìœ¨ + ë§ˆì¹¨í‘œ ë¹ˆë„
            if len(text) > 200:
                korean_chars = len([c for c in text if 'ê°€' <= c <= 'í£'])
                period_count = text.count('.') + text.count('ë‹¤.')
                score = len(text) * 0.3 + korean_chars * 0.5 + period_count * 10
                
                if score > best_score:
                    best_score = score
                    best_text = text
        
        if best_text:
            best_text = clean_article_text(best_text)
            if validate_content(best_text):
                extraction_log.append(f"âœ“ ìµœëŒ€ í…ìŠ¤íŠ¸ ë¸”ë¡ ì„±ê³µ (ì ìˆ˜: {best_score:.0f})")
                print(f"  [ì¶”ì¶œ] ìµœëŒ€ í…ìŠ¤íŠ¸ ë¸”ë¡ (ì ìˆ˜: {best_score:.0f})")
                return best_text
        
        # ===== Strategy 6: Meta Description í´ë°± =====
        meta_selectors = [
            ('meta[property="og:description"]', 'content'),
            ('meta[name="description"]', 'content'),
            ('meta[name="twitter:description"]', 'content'),
        ]
        
        for selector, attr in meta_selectors:
            meta = soup.select_one(selector)
            if meta and meta.get(attr):
                meta_content = meta.get(attr).strip()
                if len(meta_content) > 50:
                    extraction_log.append(f"âš  Meta fallback ì‚¬ìš©: {selector}")
                    print(f"  [ì¶”ì¶œ] Meta description í´ë°± (ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨, ìš”ì•½ë§Œ ì‚¬ìš©)")
                    return f"[ìš”ì•½] {meta_content}"
        
        # ===== ëª¨ë“  ì „ëµ ì‹¤íŒ¨ =====
        extraction_log.append("âœ— ëª¨ë“  ì „ëµ ì‹¤íŒ¨")
        print(f"  [ì‹¤íŒ¨] ëª¨ë“  ì¶”ì¶œ ì „ëµ ì‹¤íŒ¨")
        print(f"  URL: {final_url}")
        if extraction_log:
            print(f"  ì‹œë„ëœ ì „ëµ: {len(extraction_log)}ê°œ")
        
        return "ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except requests.exceptions.Timeout:
        print(f"  [ì˜¤ë¥˜] íƒ€ì„ì•„ì›ƒ: {url}")
        return "ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: ìš”ì²­ ì‹œê°„ ì´ˆê³¼"
    except requests.exceptions.RequestException as e:
        print(f"  [ì˜¤ë¥˜] ìš”ì²­ ì‹¤íŒ¨: {e}")
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    except Exception as e:
        print(f"  [ì˜¤ë¥˜] ì˜ˆì™¸ ë°œìƒ: {e}")
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"


def run_crawling_job():
    keywords = db_manager.get_active_keywords()
    
    if not keywords:
        print("í™œì„±í™”ëœ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 'ì¸ê³µì§€ëŠ¥'ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        crawl_naver_news("ì¸ê³µì§€ëŠ¥", use_ai=True, make_audio=True)
        return

    for k in keywords:
        print(f"\n>>> ê²€ìƒ‰ì–´ '{k['keyword']}' (ìš°ì„ ìˆœìœ„: {k.get('priority', 0)}) í¬ë¡¤ë§ ì‹œì‘...")
        crawl_naver_news(
            query=k['keyword'],
            keyword_id=k['id'],
            requirements=k['requirements'],
            use_ai=True,
            make_audio=True
        )

if __name__ == "__main__":
    import traceback
    
    logger.info("=" * 60)
    logger.info("í¬ë¡¤ëŸ¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    logger.info(f"ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}")
    logger.info(f"Python ê²½ë¡œ: {sys.executable}")
    logger.info(f"ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ: {os.path.abspath(__file__)}")
    
    # Graceful Shutdown ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ë“±ë¡
    signal.signal(signal.SIGINT, signal_handler)
    
    # Windowsì—ì„œëŠ” SIGTERMì„ ì§€ì›í•˜ì§€ ì•ŠìŒ - ìŠ¤ì¼€ì¤„ëŸ¬ í˜¸í™˜ì„±ì„ ìœ„í•´ try-except ì²˜ë¦¬
    try:
        signal.signal(signal.SIGTERM, signal_handler)
    except (OSError, AttributeError):
        pass  # Windowsì—ì„œëŠ” ë¬´ì‹œ
    
    exit_code = 0
    try:
        # Initialize DB
        logger.info("DB ì´ˆê¸°í™” ì¤‘...")
        db_manager.init_db()
        logger.info("DB ì´ˆê¸°í™” ì™„ë£Œ")
        
        logger.info(f"[{datetime.now().strftime('%H:%M:%S')}] í¬ë¡¤ëŸ¬ ì‹œì‘")
        
        # í•œ ë²ˆ ì‹¤í–‰ í›„ ì¢…ë£Œ
        run_crawling_job()
        
        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! [{datetime.now().strftime('%H:%M:%S')}]")
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
        exit_code = 1
    finally:
        logger.info(f"í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ (exit_code: {exit_code})")
        logger.info("=" * 60)
        # ëª…ì‹œì  ì¢…ë£Œ ì½”ë“œ ë°˜í™˜ (ìŠ¤ì¼€ì¤„ëŸ¬ í˜¸í™˜ì„±)
        sys.exit(exit_code)

