import requests
from bs4 import BeautifulSoup
import urllib.parse
import os
import time
import random

def crawl_naver_news(query, keyword_id=None, requirements=None, use_ai=True, make_audio=True, max_articles=3):
    # Encode the query for the URL
    encoded_query = urllib.parse.quote(query)
    
    # Base URL provided by the user
    # Note: query parameter is replaced with the user input
    url = f"https://search.naver.com/search.naver?ssc=tab.news.all&query={encoded_query}&sm=tab_opt&sort=1&nso=so%3Add"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
    }
    
    # Statistics tracking
    stats = {
        'total': 0,
        'success': 0,
        'duplicate': 0,
        'failed': 0
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Naver News search results list
        news_list_ul = soup.select_one("ul.list_news")
        
        print(f"ê²€ìƒ‰ì–´ '{query}'ì— ëŒ€í•œ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì…ë‹ˆë‹¤.\n")
        
        if not news_list_ul:                        
            print("ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return stats


        # Find headlines with fallback strategies (ë‚´êµ¬ì„± í–¥ìƒ)
        headline_selectors = [
            lambda c: c and 'sds-comps-text-type-headline1' in c,  # Current selector
            lambda c: c and 'news_tit' in c,  # Legacy selector
            lambda c: c and 'title' in c.lower() and 'news' in c.lower(),  # Generic
        ]
        
        headlines = []
        for selector in headline_selectors:
            headlines = news_list_ul.find_all(class_=selector)
            if headlines:
                print(f"âœ“ í—¤ë“œë¼ì¸ ë°œê²¬ (ì…€ë ‰í„° ì „ëµ ì‚¬ìš©)")
                break

        if not headlines:
             print("ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
             return stats

        # Limit to top N articles per keyword to avoid spamming
        for i, headline in enumerate(headlines[:max_articles]):
            stats['total'] += 1
            
            try:
                # Title
                title = headline.get_text(strip=True)
                
                # Link - ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ ìš°ì„  ì‚¬ìš© (ë³¸ë¬¸ ì¶”ì¶œ ì„±ê³µë¥  í–¥ìƒ)
                link = ""
                original_link = ""
                
                # 1. ë¨¼ì € ì›ë³¸ ë§í¬ ì €ì¥
                link_el = headline.find_parent("a")
                if link_el:
                    original_link = link_el['href']
                
                # 2. ìƒìœ„ li.bx ì»¨í…Œì´ë„ˆì—ì„œ ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ íƒìƒ‰
                news_item = headline.find_parent("li")
                if news_item:
                    # news.naver.com ë§í¬ ì°¾ê¸°
                    naver_news_link = news_item.find("a", href=lambda h: h and "news.naver.com" in h)
                    if naver_news_link:
                        link = naver_news_link['href']
                        print(f"âœ… ë„¤ì´ë²„ë‰´ìŠ¤ ë§í¬ ë°œê²¬")
                
                # 3. ë„¤ì´ë²„ ë‰´ìŠ¤ ë§í¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                if not link:
                    link = original_link
                    if link:
                        print(f"âš ï¸ ì›ë³¸ ë§í¬ ì‚¬ìš© (ë„¤ì´ë²„ë‰´ìŠ¤ ì—†ìŒ)")
                
                # Check for duplicates
                if link and db_manager.is_duplicate_news(link):
                    print(f"[ì¤‘ë³µ ê±´ë„ˆë›°ê¸°] {title}")
                    stats['duplicate'] += 1
                    continue
                
                
                # Press (closest preceding press element) with fallback
                press_selectors = [
                    lambda c: c and 'sds-comps-profile-info-title-text' in c,
                    lambda c: c and 'press' in c.lower(),
                    lambda c: c and 'info_group' in c,
                ]
                
                press = "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"
                for press_selector in press_selectors:
                    press_el = headline.find_previous(class_=press_selector)
                    if press_el:
                        press = press_el.get_text(strip=True)
                        break

                print(f"ì–¸ë¡ ì‚¬: {press}")
                print(f"ì œëª©: {title}")
                print(f"ë§í¬: {link}")
                
                
                content = ""
                if link:
                    # Rate limiting: Random delay to avoid IP blocking
                    delay = random.uniform(1.0, 3.0)
                    print(f"ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì¤‘... (ëŒ€ê¸°: {delay:.1f}ì´ˆ)")
                    time.sleep(delay)
                    content = get_news_content(link)
                    # print(f"ë³¸ë¬¸:\n{content}") # Too verbose
                
                if use_ai and content and "ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" not in content:
                    print("\n[AI íŒŸìºìŠ¤íŠ¸ ëŒ€ë³¸ ìƒì„± ì¤‘...]")
                    script = generate_podcast_script(title, content, requirements=requirements)
                    print(f"--- íŒŸìºìŠ¤íŠ¸ ëŒ€ë³¸ ---\n{script[:200]}...\n---------------------")
                    
                    if make_audio:
                        print("[ì˜¤ë””ì˜¤ íŒŒì¼ ìƒì„± ì¤‘...]")
                        
                        # Ensure MP3 directory exists
                        if not os.path.exists("MP3"):
                            os.makedirs("MP3")
                            
                        # Create a safe filename
                        safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '-', '_')]).strip()[:30]
                        filename = os.path.join("MP3", f"podcast_{safe_title}_{i}.mp3")
                        
                        # Pass title to audio generator for announcement
                        audio_result = run_audio_generation(script, filename, title=title)
                        
                        # Check if audio was successfully generated
                        if not audio_result:
                            print("âŒ ì˜¤ë””ì˜¤ ìƒì„± ì‹¤íŒ¨ - ìœ íš¨í•œ ëŒ€ë³¸ì´ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œ ë° DB ì €ì¥ ê±´ë„ˆëœ€.")
                            stats['failed'] += 1
                        else:
                            # âœ… íŒŒì¼ í¬ê¸° ì´ì¤‘ ê²€ì¦ (ì•ˆì „ì¥ì¹˜)
                            try:
                                file_size = os.path.getsize(filename)
                                file_size_mb = file_size / (1024 * 1024)
                                
                                if file_size < 1048576:  # 1MB = 1048576 bytes
                                    print(f"âŒ íŒŒì¼ í¬ê¸° ë¶€ì¡±: {file_size_mb:.2f}MB (ìµœì†Œ 1MB í•„ìš”)")
                                    print(f"   ì—…ë¡œë“œ ë° DB ë“±ë¡ ê±´ë„ˆëœ€")
                                    if os.path.exists(filename):
                                        os.remove(filename)
                                        print(f"   ë¡œì»¬ íŒŒì¼ ì‚­ì œ: {filename}")
                                    stats['failed'] += 1
                                    continue
                                
                                print(f"âœ… íŒŒì¼ í¬ê¸° ê²€ì¦ í†µê³¼: {file_size_mb:.2f}MB")
                            except Exception as e:
                                print(f"âŒ íŒŒì¼ í¬ê¸° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                                stats['failed'] += 1
                                continue
                            
                            print("[ì„œë²„ë¡œ ì—…ë¡œë“œ ì¤‘...]")
                            remote_path = upload_file(filename)
                            
                            if remote_path:
                                print(f"[DB ì €ì¥ ì¤‘...] {title}")
                                db_manager.insert_episode(press, title, link, remote_path, keyword_id=keyword_id)
                                
                                # Clean up local file after successful upload
                                try:
                                    os.remove(filename)
                                    print(f"[ë¡œì»¬ íŒŒì¼ ì‚­ì œ] {filename}")
                                except Exception as e:
                                    print(f"[ë¡œì»¬ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨] {e}")
                                
                                stats['success'] += 1
                            else:
                                print("[ì—…ë¡œë“œ ì‹¤íŒ¨] ë¡œì»¬ íŒŒì¼ ìœ ì§€")
                                stats['failed'] += 1
                else:
                    print("[ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” AI ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°]")
                    stats['failed'] += 1

                print("-" * 50)
                
            except Exception as e:
                print(f"[ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜] {e}")
                stats['failed'] += 1
                print("-" * 50)
                continue  # Move to next article
            
    except requests.exceptions.RequestException as e:
        print(f"ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    # Print statistics
    print(f"\nğŸ“Š í¬ë¡¤ë§ í†µê³„ - ì´: {stats['total']}, ì„±ê³µ: {stats['success']}, ì¤‘ë³µ: {stats['duplicate']}, ì‹¤íŒ¨: {stats['failed']}\n")
    return stats

def validate_content(content):
    """Validate article content quality."""
    if not content or len(content) < 200:
        return False
    
    # Check Korean character ratio
    korean_chars = len([c for c in content if 'ê°€' <= c <= 'í£'])
    total_chars = len(content.replace('\n', '').replace(' ', ''))
    
    if total_chars == 0:
        return False
    
    if korean_chars / total_chars < 0.3:  # At least 30% Korean
        return False
    
    return True

import re

def clean_article_text(text):
    """Remove reporter info, copyright, and other unwanted patterns from article text."""
    if not text:
        return text
    
    # ê¸°ì ì´ë©”ì¼ ì œê±°
    text = re.sub(r'\w+@\w+\.\w+', '', text)
    
    # ê¸°ì ì„œëª… íŒ¨í„´ ì œê±°
    reporter_patterns = [
        r'.*?ê¸°ì\s*=\s*',
        r'.*?íŠ¹íŒŒì›\s*=\s*', 
        r'\[.*?ê¸°ì\]',
        r'ê¸°ì\s+\w+',
    ]
    for pattern in reporter_patterns:
        text = re.sub(pattern, '', text)
    
    # ì €ì‘ê¶Œ/ì¶œì²˜ ê´€ë ¨ ì œê±°
    copyright_patterns = [
        r'Copyright\s*Â©.*',
        r'ì €ì‘ê¶Œì.*',
        r'ë¬´ë‹¨\s*ì „ì¬.*',
        r'ë°°í¬\s*ê¸ˆì§€.*',
        r'â“’.*',
    ]
    for pattern in copyright_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # SNS ê³µìœ  ê´€ë ¨ ì œê±°
    text = re.sub(r'(ì¹´ì¹´ì˜¤í†¡|í˜ì´ìŠ¤ë¶|íŠ¸ìœ„í„°|ê³µìœ í•˜ê¸°).*', '', text)
    
    # ì—¬ëŸ¬ ì¤„ë°”ê¿ˆì„ 2ê°œë¡œ ì •ë¦¬
    text = re.sub(r'\n{3,}', '\n\n', text)
    
    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def get_news_content(url):
    """Extract news article content with improved strategies."""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        # Detect encoding
        response.encoding = response.apparent_encoding
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Remove unwanted elements (enhanced)
        for unwanted in soup([
            "script", "style", "header", "footer", "nav", 
            "iframe", "noscript", "aside"  # Added aside for sidebars
        ]):
            unwanted.decompose()
        
        # Remove ads and banners
        for ad in soup.find_all(class_=lambda c: c and ('ad' in c.lower() or 'banner' in c.lower())):
            ad.decompose()
        
        # Remove Naver news specific unwanted elements
        naver_unwanted_classes = [
            'end_photo_org',  # ì‚¬ì§„ ì €ì‘ê¶Œ
            'journalist',  # ê¸°ì ì •ë³´
            'byline',  # ì¶œì²˜
            'copyright',  # ì €ì‘ê¶Œ
            'article_sponsor',  # ê´‘ê³ 
            'relation_lst',  # ê´€ë ¨ê¸°ì‚¬
            'categorize',  # ì¹´í…Œê³ ë¦¬
        ]
        
        for class_name in naver_unwanted_classes:
            for element in soup.find_all(class_=lambda c: c and class_name in c.lower()):
                element.decompose()
        
        # Remove related articles sections
        related_keywords = ['ê´€ë ¨ê¸°ì‚¬', 'í•¨ê»˜ ë³´ë©´', 'ì´ì „ ê¸°ì‚¬', 'ë‹¤ìŒ ê¸°ì‚¬', 'ì¶”ì²œ ê¸°ì‚¬', 'ì¸ê¸°ê¸°ì‚¬']
        for element in soup.find_all(['div', 'section', 'aside']):
            if element.get_text():
                text_sample = element.get_text()[:50]
                if any(keyword in text_sample for keyword in related_keywords):
                    element.decompose()
        
        content = []
        
        # Strategy 1: Try to find article tag first (common in modern news sites)
        article = soup.find('article')
        if article:
            text = article.get_text(separator='\n', strip=True)
            text = clean_article_text(text)
            if validate_content(text):
                return text
        
        # Strategy 2: Try common news content classes/ids
        content_selectors = [
            {'id': 'articleBodyContents'},  # Naver news
            {'id': 'articeBody'},
            {'class_': 'article_body'},
            {'class_': 'article-body'},
            {'id': 'newsct_article'},
        ]
        
        for selector in content_selectors:
            element = soup.find('div', selector)
            if element:
                text = element.get_text(separator='\n', strip=True)
                text = clean_article_text(text)
                if validate_content(text):
                    return text
            
        # Strategy 3: Extract all p tags (fallback)
        paragraphs = soup.find_all('p')
        for p in paragraphs:
            text = p.get_text(strip=True)
            if len(text) > 30:  # Filter out short texts (menus, copyrights, etc.)
                content.append(text)
        
        if content:
            combined = "\n".join(content)
            combined = clean_article_text(combined)
            if validate_content(combined):
                return combined
            
        return "ë³¸ë¬¸ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        return f"ë³¸ë¬¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

from datetime import datetime, timedelta 

from podcast_generator import generate_podcast_script
from podcast_audio import run_audio_generation
from sftp_uploader import upload_file
import db_manager

def run_crawling_job():
    keywords = db_manager.get_active_keywords()
    
    if not keywords:
        print("í™œì„±í™”ëœ ê²€ìƒ‰ì–´ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 'ì¸ê³µì§€ëŠ¥'ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        crawl_naver_news("ì¸ê³µì§€ëŠ¥", use_ai=True, make_audio=True)
        return

    for k in keywords:
        print(f"\n>>> ê²€ìƒ‰ì–´ '{k['keyword']}' (ìš°ì„ ìˆœìœ„: {k.get('priority', 0)}) í¬ë¡¤ë§ ì‹œì‘...")
        crawl_naver_news(
            query=k['keyword'],
            keyword_id=k['id'],
            requirements=k['requirements'],
            use_ai=True,
            make_audio=True
        )

if __name__ == "__main__":
    # Initialize DB
    db_manager.init_db()
    
    print(f"[{datetime.now().strftime('%H:%M:%S')}] í¬ë¡¤ëŸ¬ ì‹œì‘.")
    
    # Run once and exit (Windows Task Scheduler will re-run every hour)
    run_crawling_job()
    
    print(f"[{datetime.now().strftime('%H:%M:%S')}] í¬ë¡¤ë§ ì™„ë£Œ.")

